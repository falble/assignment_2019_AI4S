{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook represents an improved way to utilize our classifiers. It is created defining functions in Python that execute calculations on the dataframes just before performing the training and testing of the various classifiers. It requires, obviously, data analysis (that has not been included into the notebook because we did it for the other version of the file) but gives the possibility of a better and cleaner representation of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "\n",
    "# cleaning data (we already analyzed the data in the other notebooks)\n",
    "# need to perform some operations:\n",
    "\n",
    "# import data (both train and test) - convert it into pd.dataframe\n",
    "# transform datetime add two new columns\n",
    "# transform bytes\n",
    "# create ip category\n",
    "# create cum sum ip addr\n",
    "# dummy variable\n",
    "# (no split because we already have two datasets)\n",
    "# dropping useless columns\n",
    "# scaling data\n",
    "\n",
    "# define the classifiers (with best hyperparameters found in the other notebooks)\n",
    "# print the results\n",
    "\n",
    "# adding the opportunity to predict also attackType feature\n",
    "# define new classifiers \n",
    "# print again the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score, confusion_matrix, classification_report, \n",
    "                            balanced_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# importing data \n",
    "train_dataframe = pd.read_csv('../ASSIGNMENT/data/NetworkTraffic/CIDDS-001-internal-week1_10pcSample.csv')\n",
    "test_dataframe = pd.read_csv('../ASSIGNMENT/data/NetworkTraffic/CIDDS-001-internal-week2_10pcSample.csv')\n",
    "\n",
    "numeric_cols = ['Duration','final_bytes','Packets','Cum Count Src IP Addr (10 seconds)','Cum Count Dst IP Addr (10 seconds)']\n",
    "nominal_cols = ['Proto','Flags','Src IP category','Dst IP category','day-of-week','time-of-day']\n",
    "other_cols = ['Src IP Addr','Dst IP Addr','Src Pt','Dst Pt','Tos','Date first seen']\n",
    "label_cols = ['class','attackType','attackID','attackDescription']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming\n",
    "\n",
    "# datetime\n",
    "def datetime_transformator(train,test):\n",
    "    \n",
    "    for dataset in (train,test):\n",
    "        \n",
    "        dataset['Date first seen'] = pd.to_datetime(dataset['Date first seen'])\n",
    "        dataset['day-of-week'] = dataset['Date first seen'].dt.dayofweek\n",
    "        dataset['time-of-day'] = dataset['Date first seen'].dt.hour\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "\n",
    "# datetime\n",
    "def bytes_transformator(train,test):\n",
    "    \n",
    "    for dataset in (train,test):\n",
    "    \n",
    "        bytes_dataset_values = []\n",
    "        eval_dataset_bytes = []\n",
    "        \n",
    "        for value in dataset['Bytes'].astype('str').values:\n",
    "            \n",
    "            if value[-1] == 'M':\n",
    "                new = value.replace('M','* 1000000')\n",
    "                bytes_dataset_values.append(new)\n",
    "            else:\n",
    "                bytes_dataset_values.append(value)\n",
    "            \n",
    "        for value in bytes_dataset_values:\n",
    "            final = eval(value)\n",
    "            eval_dataset_bytes.append(final)\n",
    "            \n",
    "        dataset['final_bytes'] = eval_dataset_bytes\n",
    "\n",
    "    return train,test\n",
    "\n",
    "\n",
    "# working with ip\n",
    "def add_ip_columns(train,test):\n",
    "    \n",
    "    for dataset in (train,test):\n",
    "        \n",
    "        src_ip_category = []\n",
    "        \n",
    "        for ip_addr in dataset['Src IP Addr']:\n",
    "            if ip_addr == 'DNS':\n",
    "                #return 'DNS'\n",
    "                src_ip_category.append('DNS')\n",
    "            elif ip_addr == 'EXT_SERVER':\n",
    "                #return 'EXT_SERVER'\n",
    "                src_ip_category.append('EXT_SERVER')\n",
    "            elif len(ip_addr.split('.')) == 4:\n",
    "                #return 'private'\n",
    "                src_ip_category.append('private')\n",
    "            elif len(ip_addr.split('_')) == 2:\n",
    "                #return 'public'\n",
    "                src_ip_category.append('public')\n",
    "            else:\n",
    "                #return '-'\n",
    "                src_ip_category.append('-')\n",
    "        \n",
    "        dataset['Src IP category'] = src_ip_category\n",
    "        \n",
    "        dst_ip_category = []\n",
    "\n",
    "        for ip_addr in dataset['Dst IP Addr']:\n",
    "            if ip_addr == 'DNS':\n",
    "                #return 'DNS'\n",
    "                dst_ip_category.append('DNS')\n",
    "            elif ip_addr == 'EXT_SERVER':\n",
    "                #return 'EXT_SERVER'\n",
    "                    dst_ip_category.append('EXT_SERVER')\n",
    "            elif len(ip_addr.split('.')) == 4:\n",
    "                #return 'private'\n",
    "                    dst_ip_category.append('private')\n",
    "            elif len(ip_addr.split('_')) == 2:\n",
    "                #return 'public'\n",
    "                    dst_ip_category.append('public')\n",
    "            else:\n",
    "                #return '-'\n",
    "                    dst_ip_category.append('-')\n",
    "\n",
    "        dataset['Dst IP category'] = dst_ip_category\n",
    "        \n",
    "        dataset['Cum Count Src IP Addr (10 seconds)'] = dataset.groupby(['Src IP Addr', pd.Grouper(freq='10S',key='Date first seen')]).cumcount()\n",
    "        dataset['Cum Count Dst IP Addr (10 seconds)'] = dataset.groupby(['Dst IP Addr', pd.Grouper(freq='10S',key='Date first seen')]).cumcount()        \n",
    "        \n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformation administrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation administrator\n",
    "def transformation_administrator(train,test):\n",
    "    \n",
    "    datetime_transformator(train,test)\n",
    "    bytes_transformator(train,test)\n",
    "    add_ip_columns(train,test)\n",
    "    \n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Designer of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designer of X y for -class-\n",
    "\n",
    "def designer_X_y_for_class(train,test):\n",
    "    \n",
    "    global X_train,y_train,X_test,y_test\n",
    "            \n",
    "    # dropping and dummy    \n",
    "    # dropping useless columns\n",
    "\n",
    "    train = pd.get_dummies(train, columns=nominal_cols)    \n",
    "    test = pd.get_dummies(test, columns=nominal_cols)\n",
    "        \n",
    "    \n",
    "    # dividing train test stuff\n",
    "    X_train = train.drop(['Unnamed: 0','Bytes','Flows','Src IP Addr','Dst IP Addr','Src Pt','Dst Pt','Tos','Date first seen','class','attackType','attackID','attackDescription'],axis=1)\n",
    "    y_train = train['class']\n",
    "    X_test = test.drop(['Unnamed: 0','Bytes','Flows','Src IP Addr','Dst IP Addr','Src Pt','Dst Pt','Tos','Date first seen','class','attackType','attackID','attackDescription'], axis=1)\n",
    "    y_test = test['class']\n",
    "\n",
    "    # scaling numeric col\n",
    "    standard_scaler = StandardScaler().fit(X_train[numeric_cols])\n",
    "    X_train[numeric_cols] = standard_scaler.transform(X_train[numeric_cols])\n",
    "    X_test[numeric_cols] = standard_scaler.transform(X_test[numeric_cols])\n",
    "    \n",
    "    \n",
    "    return train,test,X_train,y_train,X_test,y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designer of X y for -attackType-\n",
    "def designer_X_y_for_type(train,test):\n",
    "    \n",
    "    global X_train_type,y_train_type,X_test_type,y_test_type\n",
    "            \n",
    "    # dropping and dummy    \n",
    "    # dropping useless columns\n",
    "   \n",
    "    train = pd.get_dummies(train, columns=nominal_cols)\n",
    "    test = pd.get_dummies(test, columns=nominal_cols)\n",
    "    \n",
    "    # dividing train test stuff\n",
    "    X_train_type = train.drop(['Unnamed: 0','Bytes','Flows','Src IP Addr','Dst IP Addr','Src Pt','Dst Pt','Tos','Date first seen','class','attackType','attackID','attackDescription'],axis=1)\n",
    "    y_train_type = train['attackType']\n",
    "    X_test_type = test.drop(['Unnamed: 0','Bytes','Flows','Src IP Addr','Dst IP Addr','Src Pt','Dst Pt','Tos','Date first seen','class','attackType','attackID','attackDescription'], axis=1)\n",
    "    y_test_type = test['attackType']\n",
    "\n",
    "    # scaling numeric col\n",
    "    standard_scaler = StandardScaler().fit(X_train_type[numeric_cols])\n",
    "    X_train_type[numeric_cols] = standard_scaler.transform(X_train_type[numeric_cols])\n",
    "    X_test_type[numeric_cols] = standard_scaler.transform(X_test_type[numeric_cols])\n",
    "    \n",
    "    \n",
    "    return train,test,X_train_type,y_train_type,X_test_type,y_test_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparator for -class-\n",
    "def data_preparator_for_class(train,test):\n",
    "    \n",
    "    global X_train,y_train,X_test,y_test\n",
    "    \n",
    "    transformation_administrator(train,test)\n",
    "    designer_X_y_for_class(train,test)\n",
    "    \n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparator for -attackType-\n",
    "def data_preparator_for_type(train,test):\n",
    "    \n",
    "    global X_train_type,y_train_type,X_test_type,y_test_type\n",
    "    \n",
    "    transformation_administrator(train,test)\n",
    "    designer_X_y_for_type(train,test)\n",
    "    \n",
    "    return X_train_type,y_train_type,X_test_type,y_test_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers -class-\n",
    "\n",
    "# Logistic Regression\n",
    "def logistic_regression(train,test):\n",
    "    print('\\nlogistic regression classifier:\\n')\n",
    "    global clf_lr, y_pred_lr\n",
    "    data_preparator_for_class(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_lr = LogisticRegression(verbose=True).fit(X_train,y_train)\n",
    "    y_pred_lr = clf_lr.predict(X_test)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print('predicting -class-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_lr,y_test,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test,y_pred_lr),'\\n')\n",
    "\n",
    "    return clf_lr,y_pred_lr\n",
    "\n",
    "\n",
    "# GaussianNB\n",
    "def gaussian_nb(train,test):\n",
    "    print('\\ngaussian naive bayes classifier:\\n')\n",
    "    global clf_nb, y_pred_nb\n",
    "    \n",
    "    data_preparator_for_class(train,test)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_nb = GaussianNB().fit(X_train,y_train)\n",
    "    y_pred_nb = clf_nb.predict(X_test)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('predicting -class-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_nb,y_test,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test,y_pred_nb))\n",
    "    \n",
    "    return clf_nb,y_pred_nb\n",
    "\n",
    "\n",
    "# Decision Trees\n",
    "def decision_trees(train,test):\n",
    "    print('\\ndecision tree classifier:\\n')\n",
    "    global clf_dt, y_pred_dt\n",
    "    \n",
    "    data_preparator_for_class(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_dt = DecisionTreeClassifier(max_depth=45, max_leaf_nodes=100, criterion='gini', splitter='best').fit(X_train,y_train)\n",
    "    y_pred_dt = clf_dt.predict(X_test)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print('predicting -class-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_dt,y_test,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test,y_pred_dt))\n",
    "    \n",
    "    return clf_dt,y_pred_dt\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "def random_forest(train,test):\n",
    "    print('\\nrandom forest classifier:\\n')\n",
    "    global clf_rf, y_pred_rf\n",
    "\n",
    "    data_preparator_for_class(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_rf = RandomForestClassifier(max_depth=45, max_leaf_nodes=100, criterion='gini', warm_start=True).fit(X_train,y_train)\n",
    "    y_pred_rf = clf_rf.predict(X_test)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print('predicting -class-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_rf,y_test,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test,y_pred_rf))    \n",
    "    \n",
    "    return clf_rf,y_pred_rf\n",
    "\n",
    "\n",
    "# Multi-layer Perceptron \n",
    "def mlp(train,test):\n",
    "    print('\\nmulti-layer perceptron:\\n')\n",
    "    global clf_mlp, y_pred_mlp\n",
    "    \n",
    "    data_preparator_for_class(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_mlp = MLPClassifier(hidden_layer_sizes=(200, ), activation='relu', solver='sgd',learning_rate='adaptive',verbose=True).fit(X_train,y_train)\n",
    "    y_pred_mlp = clf_mlp.predict(X_test)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time    \n",
    "    print('predicting -class-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_mlp,y_test,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test,y_pred_mlp)) \n",
    "    \n",
    "    return clf_mlp, y_pred_mlp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers -attackType- \n",
    "\n",
    "# Logistic Regression\n",
    "def logistic_regression_type(train,test):\n",
    "    print('\\nlogistic regression classifier:\\n')\n",
    "    global clf_lr_type, y_pred_type_lr\n",
    "    \n",
    "    data_preparator_for_type(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_lr_type = LogisticRegression(verbose=True).fit(X_train_type,y_train_type)\n",
    "    y_pred_type_lr = clf_lr_type.predict(X_test_type)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print('predicting -attackType-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_type_lr,y_test_type,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test_type,y_pred_type_lr),'\\n')\n",
    "\n",
    "    return clf_lr_type,y_pred_type_lr\n",
    "\n",
    "\n",
    "# GaussianNB\n",
    "def gaussian_nb_type(train,test):\n",
    "    print('\\ngaussian nb classifier:\\n')\n",
    "    global clf_nb_type, y_pred_type_nb\n",
    "    \n",
    "    data_preparator_for_type(train,test)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_nb_type = GaussianNB().fit(X_train_type,y_train_type)\n",
    "    y_pred_type_nb = clf_nb_type.predict(X_test_type)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('predicting -attackType-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_type_nb,y_test_type,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test_type,y_pred_type_nb))\n",
    "    \n",
    "    return clf_nb_type,y_pred_type_nb\n",
    "\n",
    "\n",
    "# Decision Trees\n",
    "def decision_trees_type(train,test):\n",
    "    print('\\ndecision trees classifier:\\n')\n",
    "    global clf_dt_type, y_pred_type_dt\n",
    "    \n",
    "    data_preparator_for_type(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_dt_type = DecisionTreeClassifier(max_depth=45, max_leaf_nodes=100, criterion='gini', splitter='best').fit(X_train_type,y_train_type)\n",
    "    y_pred_type_dt = clf_dt_type.predict(X_test_type)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print('predicting -attackType-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_type_dt,y_test_type,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test_type,y_pred_type_dt))\n",
    "    \n",
    "    return clf_dt_type,y_pred_type_dt\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "def random_forest_type(train,test):\n",
    "    print('\\nrandom forest classifier:\\n')\n",
    "    global clf_rf_type, y_pred_type_rf\n",
    "\n",
    "    data_preparator_for_type(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_rf_type = RandomForestClassifier(max_depth=45, max_leaf_nodes=100, criterion='gini', warm_start=True).fit(X_train_type,y_train_type)\n",
    "    y_pred_type_rf = clf_rf_type.predict(X_test_type)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print('predicting -attackType-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_type_rf,y_test_type,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test_type,y_pred_type_rf))    \n",
    "    \n",
    "    return clf_rf_type,y_pred_type_rf\n",
    "\n",
    "\n",
    "# Multi-layer Perceptron \n",
    "def mlp_type(train,test):\n",
    "    print('\\nmulti-layer perceptron:\\n')\n",
    "    global clf_mlp_type, y_pred_type_mlp\n",
    "    \n",
    "    data_preparator_for_type(train,test)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf_mlp_type = MLPClassifier(hidden_layer_sizes=(200, ), activation='relu', solver='sgd',learning_rate='adaptive',verbose=True).fit(X_train_type,y_train_type)\n",
    "    y_pred_type_mlp = clf_mlp_type.predict(X_test_type)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time    \n",
    "    print('predicting -attackType-:\\n')\n",
    "    print('elapsed time for training and predict:',elapsed_time,'seconds\\n')\n",
    "    print('classification report:\\n',classification_report(y_pred_type_mlp,y_test_type,digits=5),'\\n')\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test_type,y_pred_type_mlp)) \n",
    "    \n",
    "    return clf_mlp_type, y_pred_type_mlp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gaussian naive bayes classifier:\n",
      "\n",
      "predicting -class-:\n",
      "\n",
      "elapsed time for training and predict: 8.267875671386719 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    attacker    0.99573   0.99784   0.99678     90816\n",
      "      normal    0.99992   0.99925   0.99958    852372\n",
      "      victim    0.99466   0.99893   0.99679     87885\n",
      "\n",
      "    accuracy                        0.99910   1031073\n",
      "   macro avg    0.99677   0.99867   0.99772   1031073\n",
      "weighted avg    0.99910   0.99910   0.99910   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[ 90620    348     41]\n",
      " [    18 851731     53]\n",
      " [   178    293  87791]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " array(['normal', 'normal', 'normal', ..., 'normal', 'normal', 'normal'],\n",
       "       dtype='<U8'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_nb(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logistic regression classifier:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]predicting -class-:\n",
      "\n",
      "elapsed time for training and predict: 23.580148935317993 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    attacker    0.99599   0.99752   0.99676     90869\n",
      "      normal    0.99988   0.99950   0.99969    852123\n",
      "      victim    0.99705   0.99910   0.99808     88081\n",
      "\n",
      "    accuracy                        0.99929   1031073\n",
      "   macro avg    0.99764   0.99871   0.99817   1031073\n",
      "weighted avg    0.99929   0.99929   0.99929   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[ 90644    355     10]\n",
      " [    36 851697     69]\n",
      " [   189     71  88002]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='warn', tol=0.0001, verbose=True,\n",
       "                    warm_start=False),\n",
       " array(['normal', 'normal', 'normal', ..., 'normal', 'normal', 'normal'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "decision tree classifier:\n",
      "\n",
      "predicting -class-:\n",
      "\n",
      "elapsed time for training and predict: 8.79945158958435 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    attacker    0.99318   0.99961   0.99638     90423\n",
      "      normal    0.99996   0.99914   0.99955    852497\n",
      "      victim    0.99858   0.99982   0.99920     88153\n",
      "\n",
      "    accuracy                        0.99924   1031073\n",
      "   macro avg    0.99724   0.99952   0.99838   1031073\n",
      "weighted avg    0.99924   0.99924   0.99924   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[ 90388    617      4]\n",
      " [    26 851764     12]\n",
      " [     9    116  88137]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=45,\n",
       "                        max_features=None, max_leaf_nodes=100,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " array(['normal', 'normal', 'normal', ..., 'normal', 'normal', 'normal'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_trees(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "random forest classifier:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting -class-:\n",
      "\n",
      "elapsed time for training and predict: 8.826380729675293 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    attacker    0.99301   0.99979   0.99639     90392\n",
      "      normal    0.99999   0.99913   0.99956    852538\n",
      "      victim    0.99862   0.99997   0.99929     88143\n",
      "\n",
      "    accuracy                        0.99926   1031073\n",
      "   macro avg    0.99721   0.99963   0.99841   1031073\n",
      "weighted avg    0.99926   0.99926   0.99926   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[ 90373    633      3]\n",
      " [     8 851794      0]\n",
      " [    11    111  88140]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=45, max_features='auto', max_leaf_nodes=100,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=True),\n",
       " array(['normal', 'normal', 'normal', ..., 'normal', 'normal', 'normal'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "multi-layer perceptron:\n",
      "\n",
      "Iteration 1, loss = 0.11470936\n",
      "Iteration 2, loss = 0.02083602\n",
      "Iteration 3, loss = 0.01172246\n",
      "Iteration 4, loss = 0.00859972\n",
      "Iteration 5, loss = 0.00708244\n",
      "Iteration 6, loss = 0.00619104\n",
      "Iteration 7, loss = 0.00559910\n",
      "Iteration 8, loss = 0.00517254\n",
      "Iteration 9, loss = 0.00484813\n",
      "Iteration 10, loss = 0.00458998\n",
      "Iteration 11, loss = 0.00438191\n",
      "Iteration 12, loss = 0.00420896\n",
      "Iteration 13, loss = 0.00406204\n",
      "Iteration 14, loss = 0.00393647\n",
      "Iteration 15, loss = 0.00383012\n",
      "Iteration 16, loss = 0.00373839\n",
      "Iteration 17, loss = 0.00365971\n",
      "Iteration 18, loss = 0.00359119\n",
      "Iteration 19, loss = 0.00353017\n",
      "Iteration 20, loss = 0.00347504\n",
      "Iteration 21, loss = 0.00342539\n",
      "Iteration 22, loss = 0.00338190\n",
      "Iteration 23, loss = 0.00334176\n",
      "Iteration 24, loss = 0.00330490\n",
      "Iteration 25, loss = 0.00327129\n",
      "Iteration 26, loss = 0.00324070\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 27, loss = 0.00322084\n",
      "Iteration 28, loss = 0.00321496\n",
      "Iteration 29, loss = 0.00320940\n",
      "Iteration 30, loss = 0.00320396\n",
      "Iteration 31, loss = 0.00319848\n",
      "Iteration 32, loss = 0.00319333\n",
      "Iteration 33, loss = 0.00318814\n",
      "Iteration 34, loss = 0.00318302\n",
      "Iteration 35, loss = 0.00317804\n",
      "Iteration 36, loss = 0.00317303\n",
      "Iteration 37, loss = 0.00316817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 38, loss = 0.00316461\n",
      "Iteration 39, loss = 0.00316365\n",
      "Iteration 40, loss = 0.00316269\n",
      "Iteration 41, loss = 0.00316174\n",
      "Iteration 42, loss = 0.00316079\n",
      "Iteration 43, loss = 0.00315983\n",
      "Iteration 44, loss = 0.00315888\n",
      "Iteration 45, loss = 0.00315794\n",
      "Iteration 46, loss = 0.00315700\n",
      "Iteration 47, loss = 0.00315606\n",
      "Iteration 48, loss = 0.00315512\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 49, loss = 0.00315443\n",
      "Iteration 50, loss = 0.00315425\n",
      "Iteration 51, loss = 0.00315406\n",
      "Iteration 52, loss = 0.00315387\n",
      "Iteration 53, loss = 0.00315369\n",
      "Iteration 54, loss = 0.00315350\n",
      "Iteration 55, loss = 0.00315332\n",
      "Iteration 56, loss = 0.00315313\n",
      "Iteration 57, loss = 0.00315294\n",
      "Iteration 58, loss = 0.00315276\n",
      "Iteration 59, loss = 0.00315257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 60, loss = 0.00315243\n",
      "Iteration 61, loss = 0.00315240\n",
      "Iteration 62, loss = 0.00315236\n",
      "Iteration 63, loss = 0.00315232\n",
      "Iteration 64, loss = 0.00315229\n",
      "Iteration 65, loss = 0.00315225\n",
      "Iteration 66, loss = 0.00315221\n",
      "Iteration 67, loss = 0.00315217\n",
      "Iteration 68, loss = 0.00315214\n",
      "Iteration 69, loss = 0.00315210\n",
      "Iteration 70, loss = 0.00315206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 71, loss = 0.00315204\n",
      "Iteration 72, loss = 0.00315203\n",
      "Iteration 73, loss = 0.00315202\n",
      "Iteration 74, loss = 0.00315201\n",
      "Iteration 75, loss = 0.00315201\n",
      "Iteration 76, loss = 0.00315200\n",
      "Iteration 77, loss = 0.00315199\n",
      "Iteration 78, loss = 0.00315198\n",
      "Iteration 79, loss = 0.00315198\n",
      "Iteration 80, loss = 0.00315197\n",
      "Iteration 81, loss = 0.00315196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "predicting -class-:\n",
      "\n",
      "elapsed time for training and predict: 696.9692380428314 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    attacker    0.99620   0.99743   0.99681     90897\n",
      "      normal    0.99986   0.99952   0.99969    852085\n",
      "      victim    0.99707   0.99900   0.99803     88091\n",
      "\n",
      "    accuracy                        0.99929   1031073\n",
      "   macro avg    0.99771   0.99865   0.99818   1031073\n",
      "weighted avg    0.99929   0.99929   0.99929   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[ 90663    336     10]\n",
      " [    45 851679     78]\n",
      " [   189     70  88003]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(200,), learning_rate='adaptive',\n",
       "               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "               random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "               validation_fraction=0.1, verbose=True, warm_start=False),\n",
       " array(['normal', 'normal', 'normal', ..., 'normal', 'normal', 'normal'],\n",
       "       dtype='<U8'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logistic regression classifier:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]predicting -attackType-:\n",
      "\n",
      "elapsed time for training and predict: 31.440825700759888 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ---    0.99989   0.99948   0.99968    852152\n",
      "  bruteForce    0.00000   0.00000   0.00000         0\n",
      "         dos    0.99995   0.99984   0.99989    170530\n",
      "    pingScan    0.36965   0.68345   0.47980       139\n",
      "    portScan    0.95161   0.93892   0.94522      8252\n",
      "\n",
      "    accuracy                        0.99901   1031073\n",
      "   macro avg    0.66422   0.72434   0.68492   1031073\n",
      "weighted avg    0.99943   0.99901   0.99921   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[851707      0     15      0     80]\n",
      " [    64      0     12      0    284]\n",
      " [     7      0 170503      0      2]\n",
      " [    24      0      0     95    138]\n",
      " [   350      0      0     44   7748]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='warn', tol=0.0001, verbose=True,\n",
       "                    warm_start=False),\n",
       " array(['---', '---', '---', ..., '---', '---', '---'], dtype=object))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_type(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "decision trees classifier:\n",
      "\n",
      "predicting -attackType-:\n",
      "\n",
      "elapsed time for training and predict: 8.987946510314941 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ---    0.99997   0.99912   0.99955    852525\n",
      "  bruteForce    0.00000   0.00000   0.00000         4\n",
      "         dos    0.99960   0.99989   0.99974    170461\n",
      "    pingScan    0.36576   0.61039   0.45742       154\n",
      "    portScan    0.95407   0.97969   0.96671      7929\n",
      "\n",
      "    accuracy                        0.99904   1031073\n",
      "   macro avg    0.66388   0.71782   0.68468   1031073\n",
      "weighted avg    0.99946   0.99904   0.99924   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[851776      0     18      1      7]\n",
      " [   337      0      0      0     23]\n",
      " [    68      0 170443      0      1]\n",
      " [    33      0      0     94    130]\n",
      " [   311      4      0     59   7768]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=45,\n",
       "                        max_features=None, max_leaf_nodes=100,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " array(['---', '---', '---', ..., '---', '---', '---'], dtype=object))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_trees_type(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "random forest classifier:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting -attackType-:\n",
      "\n",
      "elapsed time for training and predict: 9.360949277877808 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ---    1.00000   0.99908   0.99954    852582\n",
      "  bruteForce    0.00000   0.00000   0.00000         1\n",
      "         dos    0.99938   0.99998   0.99968    170411\n",
      "    pingScan    0.21012   0.58065   0.30857        93\n",
      "    portScan    0.95677   0.97546   0.96602      7986\n",
      "\n",
      "    accuracy                        0.99901   1031073\n",
      "   macro avg    0.63325   0.71103   0.65476   1031073\n",
      "weighted avg    0.99949   0.99901   0.99924   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[851798      0      4      0      0]\n",
      " [   337      0      0      0     23]\n",
      " [   105      0 170407      0      0]\n",
      " [    30      0      0     54    173]\n",
      " [   312      1      0     39   7790]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=45, max_features='auto', max_leaf_nodes=100,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=True),\n",
       " array(['---', '---', '---', ..., '---', '---', '---'], dtype=object))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_type(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "multi-layer perceptron:\n",
      "\n",
      "Iteration 1, loss = 0.11059586\n",
      "Iteration 2, loss = 0.02399720\n",
      "Iteration 3, loss = 0.01508520\n",
      "Iteration 4, loss = 0.01166479\n",
      "Iteration 5, loss = 0.00985923\n",
      "Iteration 6, loss = 0.00872458\n",
      "Iteration 7, loss = 0.00792969\n",
      "Iteration 8, loss = 0.00733232\n",
      "Iteration 9, loss = 0.00686157\n",
      "Iteration 10, loss = 0.00647651\n",
      "Iteration 11, loss = 0.00615762\n",
      "Iteration 12, loss = 0.00588751\n",
      "Iteration 13, loss = 0.00565667\n",
      "Iteration 14, loss = 0.00545491\n",
      "Iteration 15, loss = 0.00527942\n",
      "Iteration 16, loss = 0.00512430\n",
      "Iteration 17, loss = 0.00498924\n",
      "Iteration 18, loss = 0.00487086\n",
      "Iteration 19, loss = 0.00476591\n",
      "Iteration 20, loss = 0.00467072\n",
      "Iteration 21, loss = 0.00458559\n",
      "Iteration 22, loss = 0.00450655\n",
      "Iteration 23, loss = 0.00443461\n",
      "Iteration 24, loss = 0.00436843\n",
      "Iteration 25, loss = 0.00430643\n",
      "Iteration 26, loss = 0.00424874\n",
      "Iteration 27, loss = 0.00419529\n",
      "Iteration 28, loss = 0.00414486\n",
      "Iteration 29, loss = 0.00409721\n",
      "Iteration 30, loss = 0.00405301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 31, loss = 0.00402529\n",
      "Iteration 32, loss = 0.00401703\n",
      "Iteration 33, loss = 0.00400885\n",
      "Iteration 34, loss = 0.00400073\n",
      "Iteration 35, loss = 0.00399273\n",
      "Iteration 36, loss = 0.00398482\n",
      "Iteration 37, loss = 0.00397701\n",
      "Iteration 38, loss = 0.00396928\n",
      "Iteration 39, loss = 0.00396149\n",
      "Iteration 40, loss = 0.00395396\n",
      "Iteration 41, loss = 0.00394656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 42, loss = 0.00394155\n",
      "Iteration 43, loss = 0.00394007\n",
      "Iteration 44, loss = 0.00393859\n",
      "Iteration 45, loss = 0.00393711\n",
      "Iteration 46, loss = 0.00393564\n",
      "Iteration 47, loss = 0.00393417\n",
      "Iteration 48, loss = 0.00393270\n",
      "Iteration 49, loss = 0.00393125\n",
      "Iteration 50, loss = 0.00392977\n",
      "Iteration 51, loss = 0.00392832\n",
      "Iteration 52, loss = 0.00392686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 53, loss = 0.00392589\n",
      "Iteration 54, loss = 0.00392559\n",
      "Iteration 55, loss = 0.00392530\n",
      "Iteration 56, loss = 0.00392501\n",
      "Iteration 57, loss = 0.00392472\n",
      "Iteration 58, loss = 0.00392443\n",
      "Iteration 59, loss = 0.00392415\n",
      "Iteration 60, loss = 0.00392386\n",
      "Iteration 61, loss = 0.00392357\n",
      "Iteration 62, loss = 0.00392328\n",
      "Iteration 63, loss = 0.00392299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 64, loss = 0.00392279\n",
      "Iteration 65, loss = 0.00392274\n",
      "Iteration 66, loss = 0.00392268\n",
      "Iteration 67, loss = 0.00392262\n",
      "Iteration 68, loss = 0.00392256\n",
      "Iteration 69, loss = 0.00392250\n",
      "Iteration 70, loss = 0.00392245\n",
      "Iteration 71, loss = 0.00392239\n",
      "Iteration 72, loss = 0.00392233\n",
      "Iteration 73, loss = 0.00392227\n",
      "Iteration 74, loss = 0.00392222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 75, loss = 0.00392218\n",
      "Iteration 76, loss = 0.00392217\n",
      "Iteration 77, loss = 0.00392215\n",
      "Iteration 78, loss = 0.00392214\n",
      "Iteration 79, loss = 0.00392213\n",
      "Iteration 80, loss = 0.00392212\n",
      "Iteration 81, loss = 0.00392211\n",
      "Iteration 82, loss = 0.00392210\n",
      "Iteration 83, loss = 0.00392208\n",
      "Iteration 84, loss = 0.00392207\n",
      "Iteration 85, loss = 0.00392206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "predicting -attackType-:\n",
      "\n",
      "elapsed time for training and predict: 743.4405753612518 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ---    0.99986   0.99948   0.99967    852119\n",
      "  bruteForce    0.00000   0.00000   0.00000         0\n",
      "         dos    0.99996   0.99965   0.99981    170565\n",
      "    pingScan    0.01946   1.00000   0.03817         5\n",
      "    portScan    0.95714   0.92951   0.94312      8384\n",
      "\n",
      "    accuracy                        0.99894   1031073\n",
      "   macro avg    0.59528   0.78573   0.59615   1031073\n",
      "weighted avg    0.99952   0.99894   0.99923   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[851679      0     47      0     76]\n",
      " [    64      0     12      0    284]\n",
      " [     3      0 170506      0      3]\n",
      " [    24      0      0      5    228]\n",
      " [   349      0      0      0   7793]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(200,), learning_rate='adaptive',\n",
       "               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "               random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "               validation_fraction=0.1, verbose=True, warm_start=False),\n",
       " array(['---', '---', '---', ..., '---', '---', '---'], dtype='<U10'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_type(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gaussian nb classifier:\n",
      "\n",
      "predicting -attackType-:\n",
      "\n",
      "elapsed time for training and predict: 8.655802011489868 seconds\n",
      "\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ---    0.98679   0.96815   0.97738    868206\n",
      "  bruteForce    0.00000   0.00000   0.00000      2865\n",
      "         dos    0.83777   1.00000   0.91173    142850\n",
      "    pingScan    0.84825   0.09620   0.17281      2266\n",
      "    portScan    0.91943   0.50289   0.65017     14886\n",
      "\n",
      "    accuracy                        0.96124   1031073\n",
      "   macro avg    0.71845   0.51345   0.54242   1031073\n",
      "weighted avg    0.96213   0.96124   0.95908   1031073\n",
      " \n",
      "\n",
      "confusion matrix:\n",
      " [[840550   2473      0   1739   7040]\n",
      " [    17      0      0     41    302]\n",
      " [ 27612      0 142850     31     19]\n",
      " [     0      0      0    218     39]\n",
      " [    27    392      0    237   7486]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " array(['---', '---', '---', ..., '---', '---', '---'], dtype='<U10'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_nb_type(train_dataframe,test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### @falble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### @gussr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### @FiloLafro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
